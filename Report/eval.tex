\section{Results}
\label{sec:eval}

\subsection{Input Graph Characteristics}

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{../good_plots/degree_distribution_10_mil.png}
	\caption{Degree distribution for a representative graph in our dataset, 
		demonstrating that the graphs we chose are power-law graphs.}
	\label{fig:degree_distribution}
\end{figure}

We selected eleven graphs to analyze. Although we originally chose several 
others, we were limited by the hardware we had the funds to rent to analyzing 
relatively small graphs. A tradeoff exists between the size of a graph, the 
number of workers assigned to handle its vertices, and the amount of memory on 
a server. First, there is a minimum amount of memory that a Giraph worker 
requires in 
order to run. This puts an upper bound on the number of workers that can be run 
on any machine. Second, there appears to be a minimum number of workers that 
can be assigned to a graph of a given size in Giraph: when too few are 
assigned, the job fails. Presumably, this is because each worker can only 
feasibly handle a certain number of vertices, although it is unclear why Giraph 
chooses to terminate jobs rather than allowing them to eventually complete. 
This factor puts a lower bound on the number of workers that can be assigned to 
a graph, which is dependent on the size of the graph. Given these constraints, 
we restricted ourselves to graphs with approximately 200 million edges or fewer.

Our graphs came from the Laboratory for Web Algorithmics (LAW)
\cite{BoVWFI, BRSLLP}. We considered using artificial graphs generated by 
Facebook's Darwini project \cite{edunov_darwini:_2016}, but eventually 
concluded that they were too large. The LAW graphs come 
from a variety of places. We selected a diverse subset of them, as shown in 
Table \ref{tab:graph_types}.

\begin{table*}[t]
	\centering
	\begin{tabular}{r|cccl}
		\textbf{Name} & \textbf{Vertices} & \textbf{Edges} & \textbf{Vertex 
		means...} & \textbf{Edge means...}\\
		\hline
		cnr-2000 & 325557 & 3216152 & Website & Hyperlink\\
		dblp-2011 & 986324 & 6707236 & Scientist & Paper collaboration \\
		enron & 69244 & 276143 & Person & Recipient of email\\
		wordassociation-2011 & 10617 & 72172 & Word & Interpreted association \\
		hollywood-2011 & 2180759 & 228985632 & Actor & Appearance in movie\\
		hollywood-2009 & 1139905 & 113891327 & Actor & Appearance in movie\\
		ljournal-2008 & 5363260 & 79023142 & User & Friend\\
		uk-2007-05@1000000 & 1000000 & 41247159 & Website & Hyperlink\\
		twitter & 41652230 & 1468365182 & User & Follower\\
		uk-2002 & 18520486 & 298113762 & Website & Hyperlink\\
	\end{tabular}
	\caption{Details of graph datasets}
	\label{tab:graph_types}
\end{table*}

We also paid attention to the distribution of the degrees of the vertices in 
the graphs that we chose. Since most natural graphs are power-law distributed, 
our graphs primarily follow that pattern. Figure \ref{fig:degree_distribution} 
shows a representative example of the degree distribution of a graph in our 
dataset. 

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{../good_plots/remote_to_all_modulo.png}
	\caption{Ratio of remote edges (requiring messages to be sent across the 
	network) to all edges for various numbers of partitions, where partitioning 
	is done by taking the modulus of the vertex ID and number of workers.}
	\label{fig:remote_to_all_mod}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{../good_plots/remote_to_all_chunked.png}
	\caption{Ratio of remote edges to all edges for various numbers of 
		partitions, where partitioning is done by segmenting the vertex IDs 
		into 
		approximately equal groups.}
	\label{fig:remote_to_all_range}
\end{figure}

We ran experiments to look at two metrics that we believe are predictors of 
performance: the number of edges given to each worker that require network 
requests, and the range of the number of edges given to each worker. We chose 
the first metric because we predict that if a worker is assigned a high 
proportion of edges that require it to make network requests, the time taken to 
run each step of computation will be much higher. Recall that in PageRank, 
every computational step requires a message to be sent along every edge. If 
that message begins and ends at vertices that are stored on the same machine, 
the overhead is much lower than it would be if the source and destination 
vertices are stored on two separate machines, thus necessitating a network 
request. Therefore, we measure the ratio between the number of local edges and 
the total edges assigned to a worker for various numbers of partitions and two 
partitioning schemes. Figures \ref{fig:remote_to_all_mod} and 
\ref{fig:remote_to_all_range} show the results.

Figure \ref{fig:remote_to_all_mod} shows the partitioning scheme where vertices 
are assigned to workers by taking the modulo of their vertex ID with the number 
of workers in the cluster. Figure \ref{fig:remote_to_all_range} shows the 
results of ranged partitioning. We hypothesized that 


note that graphs take locality into acount b/c of the way they're created - 
have to traverse them somewhow
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{../good_plots/range_as_cv_modulo.png}
	\caption{Range of the maximum number of edges handled by each worker to the 
		minimum handled by any worker, expressed by the coefficient of 
		variation, 
		for various numbers of partitions, with modulo partitioning.}
	\label{fig:cv_mod}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{../good_plots/range_as_cv_modulo.png}
	\caption{Range of the maximum number of edges handled by each worker to the 
		minimum handled by any worker, expressed by the coefficient of 
		variation, for various numbers of partitions, with segmented 
		partitioning.}
	\label{fig:cv_range}
\end{figure}
